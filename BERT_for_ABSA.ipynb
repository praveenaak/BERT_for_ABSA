{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LorenzoAgnolucci/BERT_for_ABSA/blob/master/BERT_for_ABSA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnQa1GPcoIOJ"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# BERT-pair\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ncOhC8Aoc6pp"
      },
      "outputs": [],
      "source": [
        "#@title Choose a dataset and a task { run: \"auto\", display-mode: \"form\" }\n",
        "base_dir = \"C:\\\\Users\\\\prave\\\\Repos\\\\BERT_for_ABSA\\\\\" #@param {type:\"string\"}\n",
        "dataset_type = \"sentihood\" #@param [\"sentihood\", \"semeval2014\"]\n",
        "task = \"NLI_M\" #@param [\"QA_M\", \"NLI_M\", \"QA_B\", \"NLI_B\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dlOFFHdiGlSw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "if dataset_type == \"sentihood\":\n",
        "    id2label = {0: \"None\", 1: \"Positive\", 2: \"Negative\"}\n",
        "    label2id = {\"None\": 0, \"Positive\": 1, \"Negative\": 2}\n",
        "elif dataset_type == \"semeval2014\":\n",
        "    id2label = {0: \"positive\", 1: \"neutral\", 2: \"negative\", 3: \"conflict\", 4: \"none\"}\n",
        "    label2id = {\"positive\": 0, \"neutral\" : 1, \"negative\" : 2, \"conflict\": 3, \"none\": 4}\n",
        "\n",
        "if task.endswith(\"B\"):\n",
        "    num_classes = 2\n",
        "else:\n",
        "    if dataset_type == \"sentihood\":\n",
        "        num_classes = 3\n",
        "    elif dataset_type == \"semeval2014\":\n",
        "        num_classes = 5\n",
        "\n",
        "def get_dataset(path):\n",
        "    original_sentences = []\n",
        "    auxiliary_sentences = []\n",
        "    labels = []\n",
        "    data = pd.read_csv(path, header=0, sep=\"\\t\").values.tolist()\n",
        "    for row in data:\n",
        "        original_sentences.append(row[1])\n",
        "        auxiliary_sentences.append(row[2])\n",
        "        labels.append(row[3])\n",
        "    return original_sentences, auxiliary_sentences, labels\n",
        "\n",
        "train_original_sentences, train_auxiliary_sentences, train_labels = get_dataset(f\"{base_dir}\\\\data\\\\{dataset_type}\\\\BERT-pair\\\\train_{task}.csv\")\n",
        "if dataset_type == \"sentihood\":\n",
        "    val_original_sentences, val_auxiliary_sentences, val_labels = get_dataset(f\"{base_dir}\\\\data\\\\{dataset_type}\\\\BERT-pair\\\\dev_{task}.csv\")\n",
        "elif dataset_type == \"semeval2014\":\n",
        "    val_original_sentences, val_auxiliary_sentences, val_labels = get_dataset(f\"{base_dir}\\\\data\\\\{dataset_type}\\\\BERT-pair\\\\test_{task}.csv\")\n",
        "test_original_sentences, test_auxiliary_sentences, test_labels = get_dataset(f\"{base_dir}\\\\data\\\\{dataset_type}\\\\BERT-pair\\\\test_{task}.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MhieorAyUUhg"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\prave\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "train_encodings = tokenizer(train_original_sentences, train_auxiliary_sentences, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_original_sentences, val_auxiliary_sentences, truncation=True, padding=True)\n",
        "test_encodings = tokenizer(test_original_sentences, test_auxiliary_sentences, truncation=True, padding=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sQV80Jd-gQBq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class ABSA_Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = ABSA_Dataset(train_encodings, train_labels)\n",
        "val_dataset = ABSA_Dataset(val_encodings, val_labels)\n",
        "test_dataset = ABSA_Dataset(test_encodings, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hLMesy14Re3T"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "if base_dir not in sys.path:\n",
        "    sys.path.insert(0, f'{base_dir}/')\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "import evaluation\n",
        "\n",
        "\n",
        "def get_test_labels(data_dir, dataset_type):\n",
        "    original_sentences = []\n",
        "    auxiliary_sentences = []\n",
        "    labels = []\n",
        "    data = pd.read_csv(f\"{data_dir}/{dataset_type}/BERT-pair/test_NLI_M.csv\", header=0, sep=\"\\t\").values.tolist()\n",
        "    for row in data:\n",
        "        labels.append(row[3])\n",
        "    return labels\n",
        "\n",
        "\n",
        "def get_predictions(data, task, dataset_type):\n",
        "    predicted_labels = []\n",
        "    scores = []\n",
        "    if task.endswith(\"B\"):\n",
        "        if dataset_type == \"sentihood\":\n",
        "            if task.endswith(\"B\"):\n",
        "                count_aspect_rows = 0\n",
        "                current_aspect_scores = []\n",
        "                for row in data:\n",
        "                    current_aspect_scores.append(row[2])\n",
        "                    count_aspect_rows += 1\n",
        "                    if count_aspect_rows % 3 == 0:\n",
        "                        sum_current_aspect_scores = np.sum(current_aspect_scores)\n",
        "                        current_aspect_scores = [score / sum_current_aspect_scores for score in current_aspect_scores]\n",
        "                        scores.append(current_aspect_scores)\n",
        "                        predicted_labels.append(np.argmax(current_aspect_scores))\n",
        "                        current_aspect_scores = []\n",
        "        elif dataset_type == \"semeval2014\":\n",
        "            if task.endswith(\"B\"):\n",
        "                count_aspect_rows = 0\n",
        "                current_aspect_scores = []\n",
        "                for row in data:\n",
        "                    current_aspect_scores.append(row[2])\n",
        "                    count_aspect_rows += 1\n",
        "                    if count_aspect_rows % 5 == 0:\n",
        "                        sum_current_aspect_scores = np.sum(current_aspect_scores)\n",
        "                        current_aspect_scores = [score / sum_current_aspect_scores for score in current_aspect_scores]\n",
        "                        scores.append(current_aspect_scores)\n",
        "                        predicted_labels.append(np.argmax(current_aspect_scores))\n",
        "                        current_aspect_scores = []\n",
        "    return predicted_labels, scores\n",
        "\n",
        "\n",
        "if dataset_type == \"sentihood\":\n",
        "    def compute_metrics(predictions):\n",
        "        scores = [softmax(prediction) for prediction in predictions[0]]\n",
        "        predicted_labels = [np.argmax(x) for x in scores]\n",
        "        if task.endswith(\"B\"):\n",
        "            data = np.insert(scores, 0, predicted_labels, axis=1)\n",
        "            predicted_labels, scores = get_predictions(data, task, dataset_type)\n",
        "        test_labels = get_test_labels(f\"{base_dir}/data\", dataset_type)\n",
        "        metrics = {}\n",
        "        metrics[\"strict_acc\"] = evaluation.compute_sentihood_aspect_strict_accuracy(test_labels, predicted_labels)\n",
        "        metrics[\"F1\"] = evaluation.compute_sentihood_aspect_macro_F1(test_labels, predicted_labels)\n",
        "        metrics[\"aspect_AUC\"] = evaluation.compute_sentihood_aspect_macro_AUC(test_labels, scores)\n",
        "        sentiment_macro_AUC, sentiment_accuracy = evaluation.compute_sentihood_sentiment_classification_metrics(test_labels, scores)\n",
        "        metrics[\"sentiment_acc\"] = sentiment_accuracy\n",
        "        metrics[\"sentiment_AUC\"] = sentiment_macro_AUC\n",
        "        return metrics\n",
        "\n",
        "elif dataset_type == \"semeval2014\":\n",
        "    def compute_metrics(predictions):\n",
        "        scores = [softmax(prediction) for prediction in predictions[0]]\n",
        "        predicted_labels = [np.argmax(x) for x in scores]\n",
        "        if task.endswith(\"B\"):\n",
        "            data = np.insert(scores, 0, predicted_labels, axis=1)\n",
        "            predicted_labels, scores = get_predictions(data, task, dataset_type)\n",
        "        test_labels = get_test_labels(f\"{base_dir}/data\", dataset_type)\n",
        "        metrics = {}\n",
        "        p, r, f1 = evaluation.compute_semeval_PRF(test_labels, predicted_labels)\n",
        "        metrics[\"P\"] = p\n",
        "        metrics[\"R\"] = r\n",
        "        metrics[\"F1\"] = f1\n",
        "        metrics[\"4-way\"] = evaluation.compute_semeval_accuracy(test_labels, predicted_labels, scores, 4)\n",
        "        metrics[\"3-way\"] = evaluation.compute_semeval_accuracy(test_labels, predicted_labels, scores, 3)\n",
        "        metrics[\"binary\"] = evaluation.compute_semeval_accuracy(test_labels, predicted_labels, scores, 2)\n",
        "        return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "0TNnE3UesI1N"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "loading configuration file config.json from cache at C:\\Users\\prave\\.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.39.3\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file model.safetensors from cache at C:\\Users\\prave\\.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\model.safetensors\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "C:\\Users\\prave\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n",
            "Currently training with a batch size of: 24\n",
            "***** Running training *****\n",
            "  Num examples = 15,008\n",
            "  Num Epochs = 4\n",
            "  Instantaneous batch size per device = 24\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 24\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 2,504\n",
            "  Number of trainable parameters = 109,484,547\n",
            "  0%|          | 10/2504 [02:49<11:38:19, 16.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 1.0175, 'grad_norm': 12.165437698364258, 'learning_rate': 8.000000000000001e-07, 'epoch': 0.02}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 20/2504 [06:13<16:01:27, 23.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.9781, 'grad_norm': 14.183942794799805, 'learning_rate': 1.6000000000000001e-06, 'epoch': 0.03}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 30/2504 [09:55<14:49:05, 21.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.8908, 'grad_norm': 15.549015045166016, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.05}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 40/2504 [13:50<16:38:53, 24.32s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.9142, 'grad_norm': 15.271132469177246, 'learning_rate': 3.2000000000000003e-06, 'epoch': 0.06}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 50/2504 [18:03<17:12:45, 25.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.8308, 'grad_norm': 7.778471946716309, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.08}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 60/2504 [22:11<16:46:43, 24.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.6898, 'grad_norm': 4.74627161026001, 'learning_rate': 4.800000000000001e-06, 'epoch': 0.1}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|▎         | 70/2504 [26:58<19:55:22, 29.47s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.6306, 'grad_norm': 7.399055480957031, 'learning_rate': 5.600000000000001e-06, 'epoch': 0.11}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|▎         | 80/2504 [31:50<19:38:15, 29.16s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.5204, 'grad_norm': 2.0145955085754395, 'learning_rate': 6.4000000000000006e-06, 'epoch': 0.13}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4%|▎         | 90/2504 [36:41<19:23:06, 28.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.5139, 'grad_norm': 2.4655892848968506, 'learning_rate': 7.2000000000000005e-06, 'epoch': 0.14}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4%|▍         | 100/2504 [41:43<21:28:57, 32.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.6115, 'grad_norm': 5.680858612060547, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.16}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4%|▍         | 110/2504 [45:55<16:54:00, 25.41s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.521, 'grad_norm': 2.5881359577178955, 'learning_rate': 8.8e-06, 'epoch': 0.18}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|▍         | 120/2504 [50:06<16:37:52, 25.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.5502, 'grad_norm': 2.3802146911621094, 'learning_rate': 9.600000000000001e-06, 'epoch': 0.19}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|▌         | 130/2504 [54:24<17:10:27, 26.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4964, 'grad_norm': 2.8707191944122314, 'learning_rate': 1.04e-05, 'epoch': 0.21}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  6%|▌         | 140/2504 [58:43<17:00:32, 25.90s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4114, 'grad_norm': 3.038390874862671, 'learning_rate': 1.1200000000000001e-05, 'epoch': 0.22}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  6%|▌         | 150/2504 [1:03:00<16:47:26, 25.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.5181, 'grad_norm': 4.402386665344238, 'learning_rate': 1.2e-05, 'epoch': 0.24}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  6%|▋         | 160/2504 [1:07:20<16:51:26, 25.89s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.5087, 'grad_norm': 2.569650173187256, 'learning_rate': 1.2800000000000001e-05, 'epoch': 0.26}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  7%|▋         | 170/2504 [1:11:40<16:47:32, 25.90s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.5129, 'grad_norm': 5.730218887329102, 'learning_rate': 1.3600000000000002e-05, 'epoch': 0.27}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  7%|▋         | 180/2504 [1:15:59<16:43:27, 25.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4928, 'grad_norm': 3.384443998336792, 'learning_rate': 1.4400000000000001e-05, 'epoch': 0.29}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  8%|▊         | 190/2504 [1:20:17<16:35:23, 25.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4594, 'grad_norm': 4.136019706726074, 'learning_rate': 1.5200000000000002e-05, 'epoch': 0.3}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  8%|▊         | 200/2504 [1:24:35<16:27:34, 25.72s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.5071, 'grad_norm': 3.2512004375457764, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.32}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  8%|▊         | 210/2504 [1:28:55<16:38:41, 26.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4471, 'grad_norm': 2.8167428970336914, 'learning_rate': 1.6800000000000002e-05, 'epoch': 0.34}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  9%|▉         | 220/2504 [1:33:12<16:15:58, 25.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.5696, 'grad_norm': 5.386629581451416, 'learning_rate': 1.76e-05, 'epoch': 0.35}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  9%|▉         | 230/2504 [1:37:30<16:24:01, 25.96s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4915, 'grad_norm': 3.7801308631896973, 'learning_rate': 1.8400000000000003e-05, 'epoch': 0.37}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|▉         | 240/2504 [1:41:50<16:18:09, 25.92s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4188, 'grad_norm': 3.1885814666748047, 'learning_rate': 1.9200000000000003e-05, 'epoch': 0.38}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|▉         | 250/2504 [1:46:02<15:38:10, 24.97s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.49, 'grad_norm': 4.234561443328857, 'learning_rate': 2e-05, 'epoch': 0.4}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 260/2504 [1:50:13<15:35:28, 25.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4388, 'grad_norm': 3.7194483280181885, 'learning_rate': 1.9911268855368235e-05, 'epoch': 0.42}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 11%|█         | 270/2504 [1:54:23<15:34:06, 25.09s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4053, 'grad_norm': 4.035529613494873, 'learning_rate': 1.982253771073647e-05, 'epoch': 0.43}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 11%|█         | 280/2504 [1:59:02<18:44:41, 30.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3454, 'grad_norm': 14.92529010772705, 'learning_rate': 1.9733806566104706e-05, 'epoch': 0.45}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 12%|█▏        | 290/2504 [2:02:46<14:10:03, 23.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4042, 'grad_norm': 5.1077704429626465, 'learning_rate': 1.964507542147294e-05, 'epoch': 0.46}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 12%|█▏        | 300/2504 [2:06:53<15:09:58, 24.77s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3954, 'grad_norm': 3.977452039718628, 'learning_rate': 1.9556344276841174e-05, 'epoch': 0.48}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 12%|█▏        | 310/2504 [2:11:06<15:22:35, 25.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3815, 'grad_norm': 6.658669948577881, 'learning_rate': 1.9467613132209407e-05, 'epoch': 0.5}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 13%|█▎        | 320/2504 [2:15:16<15:09:09, 24.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4061, 'grad_norm': 7.621420383453369, 'learning_rate': 1.937888198757764e-05, 'epoch': 0.51}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 13%|█▎        | 330/2504 [2:19:25<15:04:43, 24.97s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4449, 'grad_norm': 2.777791976928711, 'learning_rate': 1.9290150842945875e-05, 'epoch': 0.53}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 14%|█▎        | 340/2504 [2:23:35<15:14:41, 25.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3933, 'grad_norm': 4.976013660430908, 'learning_rate': 1.920141969831411e-05, 'epoch': 0.54}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 14%|█▍        | 350/2504 [2:28:50<18:58:32, 31.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2729, 'grad_norm': 2.95953369140625, 'learning_rate': 1.9112688553682342e-05, 'epoch': 0.56}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 14%|█▍        | 360/2504 [2:33:38<17:54:22, 30.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3527, 'grad_norm': 6.336037635803223, 'learning_rate': 1.9023957409050576e-05, 'epoch': 0.58}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 15%|█▍        | 370/2504 [2:38:36<17:57:32, 30.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3013, 'grad_norm': 5.022852420806885, 'learning_rate': 1.8935226264418813e-05, 'epoch': 0.59}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 15%|█▌        | 380/2504 [2:43:38<17:38:16, 29.89s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3622, 'grad_norm': 3.506969690322876, 'learning_rate': 1.8846495119787047e-05, 'epoch': 0.61}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 16%|█▌        | 390/2504 [2:48:38<17:36:05, 29.97s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.4093, 'grad_norm': 9.562499046325684, 'learning_rate': 1.875776397515528e-05, 'epoch': 0.62}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 16%|█▌        | 400/2504 [2:53:36<17:22:32, 29.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2731, 'grad_norm': 6.805218696594238, 'learning_rate': 1.8669032830523514e-05, 'epoch': 0.64}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 16%|█▋        | 410/2504 [2:58:18<15:57:43, 27.44s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2188, 'grad_norm': 4.388709545135498, 'learning_rate': 1.8580301685891748e-05, 'epoch': 0.65}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 17%|█▋        | 420/2504 [3:03:06<16:44:13, 28.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3404, 'grad_norm': 6.490188121795654, 'learning_rate': 1.8491570541259985e-05, 'epoch': 0.67}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 17%|█▋        | 430/2504 [3:07:48<16:30:46, 28.66s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.2398, 'grad_norm': 3.8625547885894775, 'learning_rate': 1.840283939662822e-05, 'epoch': 0.69}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 17%|█▋        | 433/2504 [3:09:09<15:41:08, 27.27s/it]"
          ]
        }
      ],
      "source": [
        "from transformers import BertForSequenceClassification, Trainer, TrainingArguments, BertConfig\n",
        "\n",
        "from transformers import logging\n",
        "logging.set_verbosity_debug()\n",
        "\n",
        "epochs = 4\n",
        "batch_size = 24\n",
        "num_steps = len(train_dataset) * epochs // batch_size\n",
        "warmup_steps = num_steps // 10  # 10% of the training steps\n",
        "save_steps = num_steps // epochs    # Save a checkpoint at the end of each epoch\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = f'{base_dir}models\\\\{dataset_type}\\\\\\BERT-pair\\\\{task}\\\\',          \n",
        "    num_train_epochs = epochs,              \n",
        "    per_device_train_batch_size = batch_size,  \n",
        "    per_device_eval_batch_size = batch_size,   \n",
        "    warmup_steps = warmup_steps,   \n",
        "    weight_decay = 0.01,               \n",
        "    logging_dir = f'{base_dir}logs\\\\{dataset_type}\\\\BERT-pair\\\\{task}\\\\',            \n",
        "    logging_steps = 10,\n",
        "    evaluation_strategy = 'epoch',\n",
        "    learning_rate = 2e-5,\n",
        "    save_steps = save_steps\n",
        ")\n",
        "\n",
        "config = BertConfig.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    architectures = ['BertForSequenceClassification'],\n",
        "    hidden_size = 768,\n",
        "    num_hidden_layers = 12,\n",
        "    num_attention_heads = 12,\n",
        "    hidden_dropout_prob = 0.1,\n",
        "    num_labels = num_classes\n",
        ")    \n",
        "\n",
        "load_finetuned_model = False\n",
        "if not load_finetuned_model:\n",
        "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,                         \n",
        "        args=training_args,                  \n",
        "        train_dataset=train_dataset,         \n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics             \n",
        "    )\n",
        "    trainer.train()\n",
        "\n",
        "    model.save_pretrained(f\"{base_dir}models\\\\{dataset_type}\\\\BERT-pair\\\\{task}\\\\last_step\")\n",
        "\n",
        "else:\n",
        "    model = BertForSequenceClassification.from_pretrained(f\"{base_dir}models\\\\{dataset_type}\\\\BERT-pair\\\\{task}\\\\last_step\")\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,                         \n",
        "        args=training_args,                  \n",
        "        train_dataset=train_dataset,         \n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics             \n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluation_result = trainer.evaluate(test_dataset)\n",
        "print(evaluation_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# BERT-pair\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOr0LtJdbPjM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "results = trainer.predict(test_dataset)\n",
        "\n",
        "scores = [softmax(prediction) for prediction in results.predictions]\n",
        "predicted_labels = [np.argmax(x) for x in scores]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUgiS6XM-lNE"
      },
      "outputs": [],
      "source": [
        "csv_output = np.insert(scores, 0, predicted_labels, axis=1)\n",
        "df = pd.DataFrame(csv_output)\n",
        "df[0] = df[0].astype(\"int\")\n",
        "if task.endswith(\"B\"):\n",
        "    header = [\"predicted_label\", \"no\", \"yes\"]\n",
        "else:\n",
        "    header = [\"predicted_label\"]\n",
        "    for label in label2id.keys():\n",
        "        header.append(label)\n",
        "df.to_csv(f\"{base_dir}/results/{dataset_type}/BERT-pair/{task}.csv\", index=False, header=header)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7t8j0FJWhV6R"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "if base_dir not in sys.path:\n",
        "    sys.path.insert(0, f'{base_dir}/')\n",
        "import evaluation\n",
        "evaluation.main(task, dataset_type, f\"{base_dir}/data\", f\"{base_dir}/results\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DVoml__ohye"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# BERT-single\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Us9ezWVjqKBd"
      },
      "outputs": [],
      "source": [
        "#@title Choose a dataset and a task { run: \"auto\", display-mode: \"form\" }\n",
        "base_dir = \"/gdrive/MyDrive/Machine_Learning\" #@param {type:\"string\"}\n",
        "dataset_type = \"semeval2014\" #@param [\"sentihood\", \"semeval2014\"]\n",
        "task = \"single\" #@param [\"single\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huv7NHH0yh5f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "\n",
        "if dataset_type == \"sentihood\":\n",
        "    id2label = {0: \"None\", 1: \"Positive\", 2: \"Negative\"}\n",
        "    label2id = {\"None\": 0, \"Positive\": 1, \"Negative\": 2}\n",
        "elif dataset_type == \"semeval2014\":\n",
        "    id2label = {0: \"positive\", 1: \"neutral\", 2: \"negative\", 3: \"conflict\", 4: \"none\"}\n",
        "    label2id = {\"positive\": 0, \"neutral\" : 1, \"negative\" : 2, \"conflict\": 3, \"none\": 4}\n",
        "\n",
        "if dataset_type == \"sentihood\":\n",
        "    num_classes = 3\n",
        "    locations = [\"location_1_\", \"location_2_\"]\n",
        "    aspects = [\"general\", \"price\", \"safety\", \"transit location\"]\n",
        "elif dataset_type == \"semeval2014\":\n",
        "    num_classes = 5\n",
        "    locations = [\"\"]\n",
        "    aspects = [\"ambience\", \"anecdotes\", \"food\", \"price\", \"service\"]\n",
        "\n",
        "\n",
        "def get_dataset(path):\n",
        "    original_sentences = []\n",
        "    labels = []\n",
        "    data = pd.read_csv(path, header=0, sep=\"\\t\").values.tolist()\n",
        "    for row in data:\n",
        "        original_sentences.append(row[1])\n",
        "        labels.append(row[3])\n",
        "    return original_sentences, labels\n",
        "\n",
        "\n",
        "train_original_sentences = {}\n",
        "train_labels = {}\n",
        "val_original_sentences = {}\n",
        "val_labels = {}\n",
        "test_original_sentences = {}\n",
        "test_labels = {}\n",
        "\n",
        "for location in locations:\n",
        "    train_original_sentences[location] = {}\n",
        "    train_labels[location] = {}\n",
        "    val_original_sentences[location] = {}\n",
        "    val_labels[location] = {}\n",
        "    test_original_sentences[location] = {}\n",
        "    test_labels[location] = {}\n",
        "    for aspect in aspects:\n",
        "        train_original_sentences[location][aspect], train_labels[location][aspect] = get_dataset(f\"{base_dir}/data/{dataset_type}/BERT-single/{location}{aspect}/train.csv\")\n",
        "        if dataset_type == \"sentihood\":\n",
        "            val_original_sentences[location][aspect], val_labels[location][aspect] = get_dataset(f\"{base_dir}/data/{dataset_type}/BERT-single/{location}{aspect}/dev.csv\")\n",
        "        elif dataset_type == \"semeval2014\":\n",
        "            val_original_sentences[location][aspect], val_labels[location][aspect] = get_dataset(f\"{base_dir}/data/{dataset_type}/BERT-single/{location}{aspect}/test.csv\")\n",
        "        test_original_sentences[location][aspect], test_labels[location][aspect] = get_dataset(f\"{base_dir}/data/{dataset_type}/BERT-single/{location}{aspect}/test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smEGjmdiy90j"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "train_encodings = {}\n",
        "val_encodings = {}\n",
        "test_encodings = {}\n",
        "for location in locations:\n",
        "    train_encodings[location] = {}\n",
        "    val_encodings[location] = {}\n",
        "    test_encodings[location] = {}\n",
        "    for aspect in aspects:\n",
        "        train_encodings[location][aspect] = tokenizer(train_original_sentences[location][aspect], truncation=True, padding=True)\n",
        "        val_encodings[location][aspect] = tokenizer(val_original_sentences[location][aspect], truncation=True, padding=True)\n",
        "        test_encodings[location][aspect] = tokenizer(test_original_sentences[location][aspect], truncation=True, padding=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_FhmCkKzGDs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class ABSA_Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "\n",
        "train_dataset = {}\n",
        "val_dataset = {}\n",
        "test_dataset = {}\n",
        "for location in locations:\n",
        "    train_dataset[location] = {}\n",
        "    val_dataset[location] = {}\n",
        "    test_dataset[location] = {}\n",
        "    for aspect in aspects:\n",
        "        train_dataset[location][aspect] = ABSA_Dataset(train_encodings[location][aspect], train_labels[location][aspect])\n",
        "        val_dataset[location][aspect] = ABSA_Dataset(val_encodings[location][aspect], val_labels[location][aspect])\n",
        "        test_dataset[location][aspect] = ABSA_Dataset(test_encodings[location][aspect], test_labels[location][aspect])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zubkNmxFzLTy"
      },
      "outputs": [],
      "source": [
        "from transformers import BertForSequenceClassification, Trainer, TrainingArguments, BertConfig\n",
        "from transformers import logging\n",
        "import gc\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "\n",
        "logging.set_verbosity_debug()\n",
        "\n",
        "epochs = 4\n",
        "batch_size = 24\n",
        "\n",
        "header = [\"predicted_label\"]\n",
        "for label in label2id.keys():\n",
        "    header.append(label)\n",
        "\n",
        "config = BertConfig.from_pretrained(\n",
        "        'bert-base-uncased',\n",
        "        architectures = ['BertForSequenceClassification'],\n",
        "        hidden_size = 768,\n",
        "        num_hidden_layers = 12,\n",
        "        num_attention_heads = 12,\n",
        "        hidden_dropout_prob = 0.1,\n",
        "        num_labels = num_classes\n",
        "    )    \n",
        "\n",
        "for location in locations:\n",
        "    for aspect in aspects:\n",
        "        num_steps = len(train_dataset[location][aspect]) * epochs // batch_size\n",
        "        warmup_steps = num_steps // 10  # 10% of the training steps\n",
        "        save_steps = num_steps // epochs    # Save a checkpoint at the end of each epoch\n",
        "\n",
        "\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir = f'{base_dir}/models/{dataset_type}/BERT-single/{location}{aspect}/',          \n",
        "            num_train_epochs = epochs,              \n",
        "            per_device_train_batch_size = batch_size,  \n",
        "            per_device_eval_batch_size = batch_size,   \n",
        "            warmup_steps = warmup_steps,   \n",
        "            weight_decay = 0.01,               \n",
        "            logging_dir = f'{base_dir}/logs/{dataset_type}/BERT-single/{location}{aspect}/',            \n",
        "            logging_steps = 10,\n",
        "            evaluation_strategy = 'epoch',\n",
        "            learning_rate = 2e-5,\n",
        "            save_steps = save_steps,\n",
        "            seed=21\n",
        "        )\n",
        "\n",
        "        model = BertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=model,                         \n",
        "            args=training_args,                  \n",
        "            train_dataset=train_dataset[location][aspect],         \n",
        "            eval_dataset=val_dataset[location][aspect]             \n",
        "        )\n",
        "\n",
        "        trainer.train()\n",
        "\n",
        "        model.save_pretrained(f\"{base_dir}/models/{dataset_type}/BERT-single/{location}{aspect}/last_step\")\n",
        "\n",
        "        results = trainer.predict(test_dataset[location][aspect])\n",
        "\n",
        "        scores = [softmax(prediction) for prediction in results.predictions]\n",
        "        predicted_labels = [np.argmax(x) for x in scores]\n",
        "\n",
        "        csv_output = np.insert(scores, 0, predicted_labels, axis=1)\n",
        "        df = pd.DataFrame(csv_output)\n",
        "        df[0] = df[0].astype(\"int\")\n",
        "        df.to_csv(f\"{base_dir}/results/{dataset_type}/BERT-single/{location}{aspect}.csv\", index=False, header=header)\n",
        "\n",
        "        del training_args\n",
        "        del model\n",
        "        del trainer\n",
        "        del results\n",
        "        del scores\n",
        "        del predicted_labels\n",
        "        del csv_output\n",
        "        del df\n",
        "        gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XE4Ts6ezgaT"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "if base_dir not in sys.path:\n",
        "    sys.path.insert(0, f'{base_dir}/')\n",
        "\n",
        "import evaluation\n",
        "evaluation.main(task, dataset_type, f\"{base_dir}/data\", f\"{base_dir}/results\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "location = \"location_1_\"\n",
        "aspect = \"general\"\n",
        "\n",
        "# Accessing tokenized training data\n",
        "tokenized_data = train_encodings[location][aspect]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "input_ids = torch.tensor(tokenized_data['input_ids'])\n",
        "attention_mask = torch.tensor(tokenized_data['attention_mask'])\n",
        "\n",
        "inputs = {\n",
        "    'input_ids': input_ids.unsqueeze(0),  # Add batch dimension if not already present\n",
        "    'attention_mask': attention_mask.unsqueeze(0)  # Add batch dimension if not already present\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with torch.no_grad():  # Disable gradient calculation to save memory and computations\n",
        "    outputs = model(**inputs)\n",
        "    embeddings = outputs.last_hidden_state"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "BERT_for_ABSA.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
